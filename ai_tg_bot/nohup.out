INFO:root:ðŸ”¥ Ð¿ÑƒÑƒÑƒÐº
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
    reply = await handler.handle(chat_id, text)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    answer = self.llm.chat(context)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
    response = self.client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
    reply = await handler.handle(chat_id, text)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    answer = self.llm.chat(context)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
    response = self.client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
INFO:telethon.client.updates:Got difference for account updates
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
    reply = await handler.handle(chat_id, text)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    answer = self.llm.chat(context)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
    response = self.client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
INFO:telethon.client.updates:Got difference for account updates
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
INFO:telethon.client.updates:Got difference for account updates
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:telethon.client.updates:Got difference for account updates
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
    handler = MessageHandler(memory, llm)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
    handler = MessageHandler(memory, llm)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    query=user_text
         ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 400 Bad Request"
ERROR:telethon.client.updates:Unhandled exception on on_message
Traceback (most recent call last):
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/telethon/client/updates.py", line 574, in _dispatch_update
    await callback(event)
  File "/Users/kirill/Documents/Dev Dev Dev/shit/main.py", line 43, in on_message
    handler = MessageHandler(memory, llm)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/handler.py", line 11, in handle
    self.memory.add(chat_id, "user", user_text)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/llm.py", line 12, in chat
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py", line 1192, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kirill/Documents/Dev Dev Dev/shit/.venv/lib/python3.11/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': 'Error rendering prompt with jinja template: "Conversation roles must alternate user/assistant/user/assistant/...".\n\nThis is usually an issue with the model\'s prompt template. If you are using a popular model, you can try to search the model under lmstudio-community, which will have fixed prompt templates. If you cannot find one, you are welcome to post this issue to our discord or issue tracker on GitHub. Alternatively, if you know how to write jinja templates, you can override the prompt template in My Models > model settings > Prompt Template.'}
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://127.0.0.1:1234/v1/chat/completions "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET http://localhost:8080/v1/.well-known/openid-configuration "HTTP/1.1 404 Not Found"
INFO:httpx:HTTP Request: GET http://localhost:8080/v1/meta "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://pypi.org/pypi/weaviate-client/json "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET http://localhost:8080/v1/.well-known/ready "HTTP/1.1 200 OK"
INFO:root:[WEAVIATE] Connected: True
INFO:httpx:HTTP Request: GET http://localhost:8080/v1/schema "HTTP/1.1 200 OK"
INFO:root:[AGENTIC] Memory system initialized
INFO:root:Starting bot with AgenticMemory (Weaviate + LangChain)...
INFO:root:  - Working Memory: short-term context (deque)
INFO:root:  - Episodic Memory: conversation reflections (Weaviate hybrid search)
INFO:root:  - Semantic Memory: knowledge base (Weaviate hybrid search)
INFO:root:  - Procedural Memory: behavioral rules (file-based)
INFO:root:[NOTION] ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÑŽ ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ñƒ Ñ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¼Ð¸: 2c2a7e17e02980e887cfe2f355d281e0
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c2a7e17e02980e887cfe2f355d281e0 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c2a7e17e02980e887cfe2f355d281e0/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð›Ð¾Ð²Ð¸ Ð›Ð¸Ð´Ð¾Ð²
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-8050-ba99-d6eb87986bb1 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-8050-ba99-d6eb87986bb1/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: GTM ÐœÐ°Ð½Ð¸Ñ„ÐµÑÑ‚: Ñ‡ÐµÐ¼ Ð¼Ñ‹ Ð²Ð°Ð¼ Ð¿Ð¾Ð»ÐµÐ·Ð½Ñ‹?
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-80bd-9412-d678a385fa02 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-80bd-9412-d678a385fa02/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐŸÐ¾Ð´Ð¿Ð¸ÑÑ‡Ð¸ÐºÐ¸ Ð´Ð»Ñ Ñ‚ÐµÐ»ÐµÐ³Ñ€Ð°Ð¼ ÐºÐ°Ð½Ð°Ð»Ð°
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-80d0-a7ee-db9e29810c04 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-80d0-a7ee-db9e29810c04/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/291a7e17-e029-8004-9c55-c0737d903aa2 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/291a7e17-e029-8004-9c55-c0737d903aa2/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐšÐ°Ðº Ð¿Ð¸ÑÐ°Ñ‚ÑŒ Ð¾Ñ„ÐµÑ€Ñ‹?
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-8071-b605-cb5cb9cf01f4 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-8071-b605-cb5cb9cf01f4/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/291a7e17-e029-8098-905c-f4d449f39c89/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐœÑ‹ Ð² Ð¸Ð½Ñ‚ÐµÑ€Ð½ÐµÑ‚Ðµ
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/290a7e17-e029-809b-8678-dac725c99c5f "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/290a7e17-e029-809b-8678-dac725c99c5f/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/290a7e17-e029-801b-a71c-e6f80e1e8bf2/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/290a7e17-e029-80e3-9e75-e37c55deb61c/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Web-Ð›ÐµÐ½Ð´Ð¸Ð½Ð³
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/291a7e17-e029-8051-91fe-f9f2eafe987f "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/291a7e17-e029-8051-91fe-f9f2eafe987f/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð¢ÐµÐ»ÐµÐ³Ñ€Ð°Ð¼: Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2a1a7e17-e029-800f-8e17-df39a689f989 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2a1a7e17-e029-800f-8e17-df39a689f989/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð•Ð´Ð¸Ð½Ð¸Ñ†Ñ‹ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-80d1-a025-ef0d634f8b8e "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-80d1-a025-ef0d634f8b8e/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð¡Ð¾Ñ‚Ñ€ÑƒÐ´Ð½Ð¸ÐºÐ°Ð¼: Ñ‡Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ?
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-806f-8600-e91225425f7e "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-806f-8600-e91225425f7e/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐŸÑ€Ð¾Ð´Ð°Ð¶Ð¸
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-8045-819e-e1ac04f8aa79 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-8045-819e-e1ac04f8aa79/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐŸÑ€Ð¾Ð´Ð¶ÐµÐºÑ‚ Ð¼ÐµÐ½ÐµÐ´Ð¶Ð¼ÐµÐ½Ñ‚
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-803c-bc82-c4afd83db223 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-803c-bc82-c4afd83db223/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/290a7e17-e029-8020-81c8-c7a46a7c97b4/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-803c-bc82-c4afd83db223/children?start_cursor=2c8a7e17-e029-8046-a974-c20056c54860 "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð¢Ñ‹ Ð¿Ñ€Ð¾Ð´Ð¶ÐµÐºÑ‚. Ð§Ñ‚Ð¾ Ð´ÐµÐ»Ð°Ñ‚ÑŒ?
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-808d-bc2d-c0f540d265c2 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-808d-bc2d-c0f540d265c2/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/292a7e17-e029-80fa-87fa-ea5533f74071 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/292a7e17-e029-80fa-87fa-ea5533f74071/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐžÑ‚ÐºÑƒÐ´Ð° Ð¿Ð¾Ð´Ð¿Ð¸ÑÐºÐ¸ Ð±Ð¾Ñ‚
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-804f-911c-db7b98a22d79 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-804f-911c-db7b98a22d79/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð—Ð°Ð±Ñ€Ð¸Ñ„ÑƒÐ¹ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/285a7e17-e029-802a-8c53-d94b0c0f6764 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/285a7e17-e029-802a-8c53-d94b0c0f6764/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: HR
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/285a7e17-e029-8051-9965-fc86a3979e74 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/285a7e17-e029-8051-9965-fc86a3979e74/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð’Ð°ÐºÐ°Ð½ÑÐ¸Ñ Ð´Ð»Ñ Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸ÑÑ‚Ð° Facebook
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-80d2-86d0-cd21b8b1d927 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-80d2-86d0-cd21b8b1d927/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/290a7e17-e029-8073-b4e7-f448c8eabea8/children "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/290a7e17-e029-8089-b732-ec23879c78ac/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ñ‹ Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð¸
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/pages/2c8a7e17-e029-802f-8303-eb17ecbc92e1 "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: GET https://api.notion.com/v1/blocks/2c8a7e17-e029-802f-8303-eb17ecbc92e1/children "HTTP/1.1 200 OK"
INFO:root:[NOTION] Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†Ð°: ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°, ICP Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÑ€ÐµÐ°Ñ‚Ð¸Ð²Ð¾Ð²
INFO:root:[NOTION] Ð ÐµÐºÑƒÑ€ÑÐ¸Ð²Ð½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ 13 ÑÑ‚Ñ€Ð°Ð½Ð¸Ñ†
INFO:root:[NOTION] ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽ 13 Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²...
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2c2a7e17e02980e887cfe2f355d281e0
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2c8a7e17-e029-80bd-9412-d678a385fa02
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:291a7e17-e029-8004-9c55-c0737d903aa2
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2c8a7e17-e029-8071-b605-cb5cb9cf01f4
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:290a7e17-e029-809b-8678-dac725c99c5f
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:291a7e17-e029-8051-91fe-f9f2eafe987f
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2a1a7e17-e029-800f-8e17-df39a689f989
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2c8a7e17-e029-806f-8600-e91225425f7e
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 9 chunks from notion:2c8a7e17-e029-803c-bc82-c4afd83db223
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:292a7e17-e029-80fa-87fa-ea5533f74071
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:285a7e17-e029-8051-9965-fc86a3979e74
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2c8a7e17-e029-80d2-86d0-cd21b8b1d927
INFO:httpx:HTTP Request: POST http://localhost:8080/v1/objects "HTTP/1.1 200 OK"
INFO:root:[SEMANTIC] Added 1 chunks from notion:2c8a7e17-e029-802f-8303-eb17ecbc92e1
INFO:root:[NOTION] Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð² Weaviate knowledge_base
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
WARNING:telethon.network.connection.connection:Server closed the connection: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection closed while receiving data: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Closing current connection to begin reconnect...
INFO:telethon.network.mtprotosender:Connecting to 149.154.167.51:443/TcpFull...
INFO:telethon.network.connection.connection:<class 'ConnectionResetError'> during disconnect: [Errno 54] Connection reset by peer
INFO:telethon.network.mtprotosender:Connection to 149.154.167.51:443/TcpFull complete!
WARNING:telethon.client.updates:Error executing high-level request after reconnect: <class 'sqlite3.OperationalError'>: attempt to write a readonly database
